{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c14cb79",
   "metadata": {},
   "source": [
    "Imports of all libraries and variables that will be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "386e2a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 as cv\n",
    "import depthai as dai\n",
    "import numpy as np\n",
    "import blobconverter\n",
    "from scipy.spatial.transform import Rotation\n",
    "import json\n",
    "\n",
    "rgb_json = 'intrinsics_rgb.json' # Input the path to the intrinsic_rgb json file\n",
    "robot_file = 'path_to_robot_file'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "921f436e",
   "metadata": {},
   "source": [
    "Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e385c5b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def quat_to_R(q):\n",
    "    x,y,z,w = q\n",
    "    # normalize\n",
    "    s = np.linalg.norm([w,x,y,z])\n",
    "    w,x,y,z = w/s, x/s, y/s, z/s\n",
    "    R = np.array([\n",
    "        [1-2*(y*y+z*z),   2*(x*y - z*w),   2*(x*z + y*w)],\n",
    "        [2*(x*y + z*w),   1-2*(x*x+z*z),   2*(y*z - x*w)],\n",
    "        [2*(x*z - y*w),   2*(y*z + x*w),   1-2*(x*x+y*y)]\n",
    "    ])\n",
    "    return R\n",
    "\n",
    "# SHOUTOUT https://github.com/RealManRobot/hand_eye_calibration/blob/main/compute_to_hand.py\n",
    "def convert(x ,y ,z, rotation_matrix, translation_vector):\n",
    "    obj_camera_coordinates = np.array([x, y, z])\n",
    "    T_camera_to_base_effector = np.eye(4)\n",
    "    T_camera_to_base_effector[:3, :3] = rotation_matrix\n",
    "    T_camera_to_base_effector[:3, 3] = translation_vector.reshape(3)\n",
    "\n",
    "    # Compute the pose of the object against the base\n",
    "    obj_camera_coordinates_homo = np.append(obj_camera_coordinates, [1])  # Convert object coordinates to homogeneous coordinates\n",
    "    obj_base_effector_coordinates_homo = T_camera_to_base_effector.dot(obj_camera_coordinates_homo)\n",
    "    obj_base_coordinates = obj_base_effector_coordinates_homo[:3]  \n",
    "\n",
    "    rot_matrix_homo = T_camera_to_base_effector[:3, :3]\n",
    "    quaternion = Rotation.from_matrix(rot_matrix_homo).as_quat()\n",
    "\n",
    "    return list(obj_base_coordinates), list(quaternion)\n",
    "\n",
    "def get_rtvec_t2c(intrinsincs_file):\n",
    "    with open(intrinsincs_file, 'r') as intrinsic_data:\n",
    "        data = json.load(intrinsic_data)\n",
    "        rvecs = [[val[0] for val in vec] for vec in data[\"rvecs\"]]\n",
    "        tvecs = [[val[0] * 10 for val in vec] for vec in data[\"tvecs\"]] # We multiply with 10 to change the tvecs from cm to mm\n",
    "    return rvecs, tvecs   \n",
    "\n",
    "def get_robot_pos_quat(robot_file):\n",
    "    pass\n",
    "\n",
    "def camera2robot_calib(position_robot, quat_robot, rotation_vec_t2c, translation_t2c):\n",
    "    R_g2b, t_g2b = [], []\n",
    "    for pos, quat in zip(position_robot, quat_robot): #position robot list of XYZ for each position of calibration and quat is its quaternion\n",
    "        R_b2g = quat_to_R(quat)                                 # (3,3)\n",
    "        t_b2g = np.asarray(pos, np.float64).reshape(3,1)  #  (3,1)\n",
    "        # invert to gripper->base\n",
    "        R_gb = R_b2g.T\n",
    "        t_gb = - R_gb @ t_b2g\n",
    "        R_g2b.append(R_gb)\n",
    "        t_g2b.append(t_gb)\n",
    "\n",
    "    R_t2c = [cv.Rodrigues(np.array(rv, np.float64))[0] for rv in rotation_vec_t2c]\n",
    "    t_t2c = [np.array(tv, np.float64).reshape(3,1)     for tv in translation_t2c]\n",
    "\n",
    "    R_cam2gripper, t_cam2gripper = cv.calibrateHandEye(\n",
    "        R_gripper2base=R_g2b, t_gripper2base=t_g2b,\n",
    "        R_target2cam=R_t2c,   t_target2cam=t_t2c,\n",
    "        method=cv.CALIB_HAND_EYE_DANIILIDIS\n",
    "    )\n",
    "    return R_cam2gripper, t_cam2gripper\n",
    "\n",
    "def json_converter(coordinates, quaternion):\n",
    "    data = [coordinates, quaternion] # We store the coordinates and quaternion in a list so that it can be easily accessible.\n",
    "\n",
    "    # The layout/structure of the JSON file\n",
    "    cup = {\n",
    "        \"id\":\"cup_1\",\n",
    "        \"status\": \"Available\",\n",
    "        \"position\":{\n",
    "            \"x\": data[0][0],\n",
    "            \"y\": data[0][1],\n",
    "            \"z\": data[0][2]\n",
    "        },\n",
    "        \"orientation\":{\n",
    "            \"q1\": float(data[1][0]),\n",
    "            \"q2\": float(data[1][1]),\n",
    "            \"q3\": float(data[1][2]),\n",
    "            \"q4\": float(data[1][3])\n",
    "        },\n",
    "        \"approach_position\":{\n",
    "            \"x\": data[0][0],\n",
    "            \"y\": data[0][1],\n",
    "            \"z\": data[0][2]\n",
    "        }\n",
    "    }\n",
    "\n",
    "    cup_data = {\"cups\": cup}\n",
    "\n",
    "    with open(\"cups.json\", \"w\") as f:\n",
    "        json.dump(cup_data, f, indent=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f594db",
   "metadata": {},
   "source": [
    "Camera setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b850acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DepthAI pipeline\n",
    "pipeline = dai.Pipeline()\n",
    "\n",
    "# Define camera nodes\n",
    "cam_rgb   = pipeline.create(dai.node.ColorCamera)\n",
    "mono_left  = pipeline.create(dai.node.MonoCamera)\n",
    "mono_right = pipeline.create(dai.node.MonoCamera)\n",
    "stereo     = pipeline.create(dai.node.StereoDepth)\n",
    "detection_nn = pipeline.create(dai.node.YoloSpatialDetectionNetwork)  # Spatial NN for YOLO\n",
    "\n",
    "# Define XLink outputs for streaming frames and detections to host\n",
    "xout_rgb   = pipeline.create(dai.node.XLinkOut)\n",
    "xout_depth = pipeline.create(dai.node.XLinkOut)\n",
    "xout_nn    = pipeline.create(dai.node.XLinkOut)\n",
    "xout_rgb.setStreamName(\"rgb\")\n",
    "xout_depth.setStreamName(\"depth\")\n",
    "xout_nn.setStreamName(\"detections\")\n",
    "\n",
    "# Camera configuration (RGB camera)\n",
    "cam_rgb.setBoardSocket(dai.CameraBoardSocket.RGB)\n",
    "cam_rgb.setResolution(dai.ColorCameraProperties.SensorResolution.THE_1080_P)\n",
    "cam_rgb.setPreviewSize(416, 416)  # neural network input size for TinyYOLOv4\n",
    "cam_rgb.setInterleaved(False)\n",
    "cam_rgb.setColorOrder(dai.ColorCameraProperties.ColorOrder.BGR)\n",
    "\n",
    "# Mono cameras (for depth)\n",
    "mono_left.setBoardSocket(dai.CameraBoardSocket.LEFT)\n",
    "mono_right.setBoardSocket(dai.CameraBoardSocket.RIGHT)\n",
    "mono_left.setResolution(dai.MonoCameraProperties.SensorResolution.THE_400_P)\n",
    "mono_right.setResolution(dai.MonoCameraProperties.SensorResolution.THE_400_P)\n",
    "\n",
    "# Stereo depth configuration\n",
    "stereo.setDefaultProfilePreset(dai.node.StereoDepth.PresetMode.HIGH_DENSITY)\n",
    "stereo.setDepthAlign(dai.CameraBoardSocket.RGB)        # Align depth map to RGB camera perspective:contentReference[oaicite:25]{index=25}\n",
    "stereo.setOutputSize(mono_left.getResolutionWidth(), \n",
    "                     mono_left.getResolutionHeight())\n",
    "stereo.setSubpixel(True)  # improve depth precision\n",
    "\n",
    "# Download and set neural network model (Tiny-YOLOv4 COCO 416x416) <------- THIS CAN BE WHATEVER MODEL THAT EXISTS IN THE DEPTHAI ZOO\n",
    "blob_path = blobconverter.from_zoo(name=\"yolov4_tiny_coco_416x416\", \n",
    "                                   zoo_type=\"depthai\", \n",
    "                                   shaves=6)\n",
    "detection_nn.setBlobPath(str(blob_path))\n",
    "detection_nn.setConfidenceThreshold(0.5)\n",
    "detection_nn.input.setBlocking(False)\n",
    "detection_nn.setBoundingBoxScaleFactor(0.5)  # extra area for depth averaging\n",
    "detection_nn.setDepthLowerThreshold(100)     # 100 mm -> 10 cm minimum distance:contentReference[oaicite:26]{index=26}\n",
    "detection_nn.setDepthUpperThreshold(5000)    # 5000 mm -> 5 m maximum distance:contentReference[oaicite:27]{index=27}\n",
    "\n",
    "# YOLO-specific network settings (for COCO Tiny-YOLOv4 416x416)\n",
    "detection_nn.setNumClasses(80)\n",
    "detection_nn.setCoordinateSize(4)\n",
    "detection_nn.setAnchors([10,14, 23,27, 37,58, 81,82, 135,169, 344,319])       # from model config:contentReference[oaicite:28]{index=28}\n",
    "detection_nn.setAnchorMasks({ \"side26\": [1,2,3], \"side13\": [3,4,5] })         # YOLOv4-tiny masks:contentReference[oaicite:29]{index=29}\n",
    "detection_nn.setIouThreshold(0.5)\n",
    "\n",
    "# Link nodes: RGB -> Neural Network, Mono -> StereoDepth, Depth -> Neural Network\n",
    "cam_rgb.preview.link(detection_nn.input)\n",
    "mono_left.out.link(stereo.left)\n",
    "mono_right.out.link(stereo.right)\n",
    "stereo.depth.link(detection_nn.inputDepth)\n",
    "\n",
    "# Link NN outputs to XLink outputs\n",
    "detection_nn.passthrough.link(xout_rgb.input)      # passthrough RGB frames (frames that went into NN)\n",
    "detection_nn.passthroughDepth.link(xout_depth.input)  # aligned depth frames\n",
    "detection_nn.out.link(xout_nn.input)              # detection outputs (bounding boxes + coordinates)\n",
    "\n",
    "# Initialize the device and pipeline\n",
    "with dai.Device(pipeline) as device:\n",
    "    # Output queues to retrieve frames and detections\n",
    "    q_rgb   = device.getOutputQueue(name=\"rgb\", maxSize=4, blocking=False)\n",
    "    q_depth = device.getOutputQueue(name=\"depth\", maxSize=4, blocking=False)\n",
    "    q_det   = device.getOutputQueue(name=\"detections\", maxSize=4, blocking=False)\n",
    "\n",
    "    # Get label names for COCO classes (for display purposes)\n",
    "    label_map = [\n",
    "        \"person\",\"bicycle\",\"car\",\"motorbike\",\"aeroplane\",\"bus\",\"train\",\"truck\",\"boat\",\n",
    "        \"traffic light\",\"fire hydrant\",\"stop sign\",\"parking meter\",\"bench\",\"bird\",\"cat\",\n",
    "        \"dog\",\"horse\",\"sheep\",\"cow\",\"elephant\",\"bear\",\"zebra\",\"giraffe\",\"backpack\",\"umbrella\",\n",
    "        \"handbag\",\"tie\",\"suitcase\",\"frisbee\",\"skis\",\"snowboard\",\"sports ball\",\"kite\",\n",
    "        \"baseball bat\",\"baseball glove\",\"skateboard\",\"surfboard\",\"tennis racket\",\"bottle\",\n",
    "        \"wine glass\",\"cup\",\"fork\",\"knife\",\"spoon\",\"bowl\",\"banana\",\"apple\",\"sandwich\",\n",
    "        \"orange\",\"broccoli\",\"carrot\",\"hot dog\",\"pizza\",\"donut\",\"cake\",\"chair\",\"sofa\",\n",
    "        \"pottedplant\",\"bed\",\"diningtable\",\"toilet\",\"tvmonitor\",\"laptop\",\"mouse\",\"remote\",\n",
    "        \"keyboard\",\"cell phone\",\"microwave\",\"oven\",\"toaster\",\"sink\",\"refrigerator\",\"book\",\n",
    "        \"clock\",\"vase\",\"scissors\",\"teddy bear\",\"hair drier\",\"toothbrush\"\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b83930ee",
   "metadata": {},
   "source": [
    "Code that needs to be ran once before running the continous loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "760d6ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "rotation_vec_t2c, translation_t2c = get_rtvec_t2c(rgb_json)\n",
    "position_robot, quat_robot = get_robot_pos_quat(robot_file)\n",
    "\n",
    "R_cam2base, t_cam2base = camera2robot_calib(position_robot, quat_robot, rotation_vec_t2c, translation_t2c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7df4f22",
   "metadata": {},
   "source": [
    "Continously running loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d3d8915",
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    in_rgb   = q_rgb.get()      # latest RGB frame\n",
    "    in_depth = q_depth.get()    # latest depth frame (aligned to RGB)\n",
    "    in_dets  = q_det.get()      # latest detection results\n",
    "\n",
    "    frame = in_rgb.getCvFrame()               # OpenCV BGR frame from color camera\n",
    "    depth_frame = in_depth.getFrame()         # depth data in millimeters\n",
    "    detections = in_dets.detections           # list of spatial detections\n",
    "\n",
    "    # Iterate over detections and draw bounding boxes and labels (SET TO ONLY DISPLAY CUPS AT THE MOMENT)\n",
    "    for det in detections:\n",
    "        \n",
    "        # Determine label text to display\n",
    "        label = str(det.label)\n",
    "        if det.label < len(label_map):\n",
    "            label = label_map[det.label]\n",
    "        conf  = int(det.confidence * 100)  # confidence percentage\n",
    "\n",
    "        if label == \"cup\":\n",
    "            # Get bounding box coordinates (normalized 0..1 from NN, convert to pixel coords)\n",
    "            x1 = int(det.xmin * frame.shape[1])\n",
    "            y1 = int(det.ymin * frame.shape[0])\n",
    "            x2 = int(det.xmax * frame.shape[1])\n",
    "            y2 = int(det.ymax * frame.shape[0])\n",
    "\n",
    "            # Draw rectangle on RGB frame\n",
    "            cv.rectangle(frame, (x1, y1), (x2, y2), (0,255,0), 2)\n",
    "\n",
    "            # Draw label and confidence\n",
    "            cv.putText(frame, f\"{label} ({conf}%)\", (x1+5, y1+20),\n",
    "                        cv.FONT_HERSHEY_SIMPLEX, 0.5, (255,255,255), 1)\n",
    "\n",
    "            # Draw spatial coordinates (X, Y, Z in mm)\n",
    "            coords = det.spatialCoordinates  # spatial coordinates relative to camera\n",
    "            cv.putText(frame, f\"X: {int(coords.x)} mm\", (x1+5, y1+35),\n",
    "                        cv.FONT_HERSHEY_SIMPLEX, 0.5, (255,255,255), 1)\n",
    "            cv.putText(frame, f\"Y: {int(coords.y)} mm\", (x1+5, y1+50),\n",
    "                        cv.FONT_HERSHEY_SIMPLEX, 0.5, (255,255,255), 1)\n",
    "            cv.putText(frame, f\"Z: {int(coords.z)} mm\", (x1+5, y1+65),\n",
    "                        cv.FONT_HERSHEY_SIMPLEX, 0.5, (255,255,255), 1)\n",
    "\n",
    "            coordinates, quaternion = convert(coords.x,coords.y,coords.z, R_cam2base, t_cam2base) # Convert from camera coordinates to robot coordinates.\n",
    "            json_converter(coordinates, quaternion)\n",
    "            \n",
    "\n",
    "    # Show the frames in windows\n",
    "    cv.imshow(\"RGB\", frame)\n",
    "\n",
    "    # Exit on 'q' key\n",
    "    if cv.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cv.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
