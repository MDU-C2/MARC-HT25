{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1795709",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import depthai as dai\n",
    "import numpy as np\n",
    "import blobconverter\n",
    "\n",
    "# Create DepthAI pipeline\n",
    "pipeline = dai.Pipeline()\n",
    "\n",
    "# Define camera nodes\n",
    "cam_rgb   = pipeline.create(dai.node.ColorCamera)\n",
    "mono_left  = pipeline.create(dai.node.MonoCamera)\n",
    "mono_right = pipeline.create(dai.node.MonoCamera)\n",
    "stereo     = pipeline.create(dai.node.StereoDepth)\n",
    "detection_nn = pipeline.create(dai.node.YoloSpatialDetectionNetwork)  # Spatial NN for YOLO\n",
    "\n",
    "# Define XLink outputs for streaming frames and detections to host\n",
    "xout_rgb   = pipeline.create(dai.node.XLinkOut)\n",
    "xout_depth = pipeline.create(dai.node.XLinkOut)\n",
    "xout_nn    = pipeline.create(dai.node.XLinkOut)\n",
    "xout_rgb.setStreamName(\"rgb\")\n",
    "xout_depth.setStreamName(\"depth\")\n",
    "xout_nn.setStreamName(\"detections\")\n",
    "\n",
    "# Camera configuration (RGB camera)\n",
    "cam_rgb.setBoardSocket(dai.CameraBoardSocket.RGB)\n",
    "cam_rgb.setResolution(dai.ColorCameraProperties.SensorResolution.THE_1080_P)\n",
    "cam_rgb.setPreviewSize(416, 416)  # neural network input size for TinyYOLOv4\n",
    "cam_rgb.setInterleaved(False)\n",
    "cam_rgb.setColorOrder(dai.ColorCameraProperties.ColorOrder.BGR)\n",
    "\n",
    "# Mono cameras (for depth)\n",
    "mono_left.setBoardSocket(dai.CameraBoardSocket.LEFT)\n",
    "mono_right.setBoardSocket(dai.CameraBoardSocket.RIGHT)\n",
    "mono_left.setResolution(dai.MonoCameraProperties.SensorResolution.THE_400_P)\n",
    "mono_right.setResolution(dai.MonoCameraProperties.SensorResolution.THE_400_P)\n",
    "\n",
    "# Stereo depth configuration\n",
    "stereo.setDefaultProfilePreset(dai.node.StereoDepth.PresetMode.HIGH_DENSITY)\n",
    "stereo.setDepthAlign(dai.CameraBoardSocket.RGB)        # Align depth map to RGB camera perspective:contentReference[oaicite:25]{index=25}\n",
    "stereo.setOutputSize(mono_left.getResolutionWidth(), \n",
    "                     mono_left.getResolutionHeight())\n",
    "stereo.setSubpixel(True)  # improve depth precision\n",
    "\n",
    "# Download and set neural network model (Tiny-YOLOv4 COCO 416x416)\n",
    "blob_path = blobconverter.from_zoo(name=\"yolov4_tiny_coco_416x416\", \n",
    "                                   zoo_type=\"depthai\", \n",
    "                                   shaves=6)\n",
    "detection_nn.setBlobPath(str(blob_path))\n",
    "detection_nn.setConfidenceThreshold(0.5)\n",
    "detection_nn.input.setBlocking(False)\n",
    "detection_nn.setBoundingBoxScaleFactor(0.5)  # extra area for depth averaging\n",
    "detection_nn.setDepthLowerThreshold(100)     # 100 mm -> 10 cm minimum distance:contentReference[oaicite:26]{index=26}\n",
    "detection_nn.setDepthUpperThreshold(5000)    # 5000 mm -> 5 m maximum distance:contentReference[oaicite:27]{index=27}\n",
    "\n",
    "# YOLO-specific network settings (for COCO Tiny-YOLOv4 416x416)\n",
    "detection_nn.setNumClasses(80)\n",
    "detection_nn.setCoordinateSize(4)\n",
    "detection_nn.setAnchors([10,14, 23,27, 37,58, 81,82, 135,169, 344,319])       # from model config:contentReference[oaicite:28]{index=28}\n",
    "detection_nn.setAnchorMasks({ \"side26\": [1,2,3], \"side13\": [3,4,5] })         # YOLOv4-tiny masks:contentReference[oaicite:29]{index=29}\n",
    "detection_nn.setIouThreshold(0.5)\n",
    "\n",
    "# Link nodes: RGB -> Neural Network, Mono -> StereoDepth, Depth -> Neural Network\n",
    "cam_rgb.preview.link(detection_nn.input)\n",
    "mono_left.out.link(stereo.left)\n",
    "mono_right.out.link(stereo.right)\n",
    "stereo.depth.link(detection_nn.inputDepth)\n",
    "\n",
    "# Link NN outputs to XLink outputs\n",
    "detection_nn.passthrough.link(xout_rgb.input)      # passthrough RGB frames (frames that went into NN)\n",
    "detection_nn.passthroughDepth.link(xout_depth.input)  # aligned depth frames\n",
    "detection_nn.out.link(xout_nn.input)              # detection outputs (bounding boxes + coordinates)\n",
    "\n",
    "# Initialize the device and pipeline\n",
    "with dai.Device(pipeline) as device:\n",
    "    # Output queues to retrieve frames and detections\n",
    "    q_rgb   = device.getOutputQueue(name=\"rgb\", maxSize=4, blocking=False)\n",
    "    q_depth = device.getOutputQueue(name=\"depth\", maxSize=4, blocking=False)\n",
    "    q_det   = device.getOutputQueue(name=\"detections\", maxSize=4, blocking=False)\n",
    "\n",
    "    # Get label names for COCO classes (for display purposes)\n",
    "    label_map = [\n",
    "        \"person\",\"bicycle\",\"car\",\"motorbike\",\"aeroplane\",\"bus\",\"train\",\"truck\",\"boat\",\n",
    "        \"traffic light\",\"fire hydrant\",\"stop sign\",\"parking meter\",\"bench\",\"bird\",\"cat\",\n",
    "        \"dog\",\"horse\",\"sheep\",\"cow\",\"elephant\",\"bear\",\"zebra\",\"giraffe\",\"backpack\",\"umbrella\",\n",
    "        \"handbag\",\"tie\",\"suitcase\",\"frisbee\",\"skis\",\"snowboard\",\"sports ball\",\"kite\",\n",
    "        \"baseball bat\",\"baseball glove\",\"skateboard\",\"surfboard\",\"tennis racket\",\"bottle\",\n",
    "        \"wine glass\",\"cup\",\"fork\",\"knife\",\"spoon\",\"bowl\",\"banana\",\"apple\",\"sandwich\",\n",
    "        \"orange\",\"broccoli\",\"carrot\",\"hot dog\",\"pizza\",\"donut\",\"cake\",\"chair\",\"sofa\",\n",
    "        \"pottedplant\",\"bed\",\"diningtable\",\"toilet\",\"tvmonitor\",\"laptop\",\"mouse\",\"remote\",\n",
    "        \"keyboard\",\"cell phone\",\"microwave\",\"oven\",\"toaster\",\"sink\",\"refrigerator\",\"book\",\n",
    "        \"clock\",\"vase\",\"scissors\",\"teddy bear\",\"hair drier\",\"toothbrush\"\n",
    "    ]  # COCO 80-class label list (indices 0-79):contentReference[oaicite:30]{index=30}:contentReference[oaicite:31]{index=31}\n",
    "\n",
    "    while True:\n",
    "        in_rgb   = q_rgb.get()      # latest RGB frame\n",
    "        in_depth = q_depth.get()    # latest depth frame (aligned to RGB)\n",
    "        in_dets  = q_det.get()      # latest detection results\n",
    "\n",
    "        frame = in_rgb.getCvFrame()               # OpenCV BGR frame from color camera\n",
    "        depth_frame = in_depth.getFrame()         # depth data in millimeters\n",
    "        detections = in_dets.detections           # list of spatial detections\n",
    "\n",
    "        # (Optional) Colorize depth map for visualization\n",
    "        # Normalize depth for display, then apply colormap\n",
    "        depth_min = depth_frame.min()\n",
    "        depth_max = depth_frame.max()\n",
    "        depth_frame_disp = ((depth_frame - depth_min) / (depth_max - depth_min) * 255).astype(np.uint8)\n",
    "        depth_frame_disp = cv2.applyColorMap(depth_frame_disp, cv2.COLORMAP_JET)\n",
    "\n",
    "        # Iterate over detections and draw bounding boxes and labels\n",
    "        for det in detections:\n",
    "            # Get bounding box coordinates (normalized 0..1 from NN, convert to pixel coords)\n",
    "            x1 = int(det.xmin * frame.shape[1])\n",
    "            y1 = int(det.ymin * frame.shape[0])\n",
    "            x2 = int(det.xmax * frame.shape[1])\n",
    "            y2 = int(det.ymax * frame.shape[0])\n",
    "\n",
    "            # Draw rectangle on RGB frame\n",
    "            cv2.rectangle(frame, (x1, y1), (x2, y2), (0,255,0), 2)\n",
    "\n",
    "            # Determine label text to display\n",
    "            label = str(det.label)\n",
    "            if det.label < len(label_map):\n",
    "                label = label_map[det.label]\n",
    "            conf  = int(det.confidence * 100)  # confidence percentage\n",
    "\n",
    "            # Draw label and confidence\n",
    "            cv2.putText(frame, f\"{label} ({conf}%)\", (x1+5, y1+20),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255,255,255), 1)\n",
    "\n",
    "            # Draw spatial coordinates (X, Y, Z in mm)\n",
    "            coords = det.spatialCoordinates  # spatial coordinates relative to camera\n",
    "            cv2.putText(frame, f\"X: {int(coords.x)} mm\", (x1+5, y1+35),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255,255,255), 1)\n",
    "            cv2.putText(frame, f\"Y: {int(coords.y)} mm\", (x1+5, y1+50),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255,255,255), 1)\n",
    "            cv2.putText(frame, f\"Z: {int(coords.z)} mm\", (x1+5, y1+65),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255,255,255), 1)\n",
    "            # Note: X is positive to the right, Y is positive downwards, Z is depth (forward):contentReference[oaicite:32]{index=32}\n",
    "\n",
    "        # Show the frames in windows\n",
    "        cv2.imshow(\"RGB\", frame)\n",
    "        cv2.imshow(\"Depth\", depth_frame_disp)\n",
    "\n",
    "        # Exit on 'q' key\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "# Cleanup\n",
    "cv2.destroyAllWindows()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
